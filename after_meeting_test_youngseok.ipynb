{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1700, 92)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pd.read_csv('./features_reduced_imputed.csv')\n",
    "x.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. binary column중 unbalance한 Feature 삭제 therehold = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEX is balanced. 635 1065\n",
      "SIM_GIPERT is balanced. 1643 57\n",
      "nr_11 is balanced. 1658 42\n",
      "nr_01 is unbalanced. 1696 4\n",
      "nr_02 is balanced. 1681 19\n",
      "nr_03 is balanced. 1665 35\n",
      "nr_04 is balanced. 1671 29\n",
      "nr_07 is unbalanced. 1699 1\n",
      "nr_08 is unbalanced. 1696 4\n",
      "np_01 is unbalanced. 1698 2\n",
      "np_04 is unbalanced. 1697 3\n",
      "np_05 is unbalanced. 1689 11\n",
      "np_07 is unbalanced. 1699 1\n",
      "np_08 is unbalanced. 1694 6\n",
      "np_09 is unbalanced. 1698 2\n",
      "np_10 is unbalanced. 1697 3\n",
      "endocr_01 is balanced. 1472 228\n",
      "endocr_02 is balanced. 1658 42\n",
      "endocr_03 is unbalanced. 1687 13\n",
      "zab_leg_01 is balanced. 1564 136\n",
      "zab_leg_02 is balanced. 1579 121\n",
      "zab_leg_03 is balanced. 1663 37\n",
      "zab_leg_04 is unbalanced. 1691 9\n",
      "zab_leg_06 is balanced. 1678 22\n",
      "O_L_POST is balanced. 1590 110\n",
      "K_SH_POST is balanced. 1654 46\n",
      "SVT_POST is unbalanced. 1692 8\n",
      "GT_POST is unbalanced. 1692 8\n",
      "FIB_G_POST is unbalanced. 1685 15\n",
      "IM_PG_P is balanced. 1650 50\n",
      "ritm_ecg_p_04 is balanced. 1677 23\n",
      "ritm_ecg_p_06 is unbalanced. 1699 1\n",
      "ritm_ecg_p_08 is balanced. 1654 46\n",
      "n_r_ecg_p_01 is balanced. 1642 58\n",
      "n_r_ecg_p_02 is unbalanced. 1692 8\n",
      "n_r_ecg_p_03 is balanced. 1493 207\n",
      "n_r_ecg_p_04 is balanced. 1631 69\n",
      "n_r_ecg_p_05 is balanced. 1630 70\n",
      "n_r_ecg_p_06 is balanced. 1668 32\n",
      "n_r_ecg_p_08 is unbalanced. 1696 4\n",
      "n_r_ecg_p_09 is unbalanced. 1698 2\n",
      "n_r_ecg_p_10 is unbalanced. 1698 2\n",
      "n_p_ecg_p_01 is unbalanced. 1698 2\n",
      "n_p_ecg_p_03 is balanced. 1668 32\n",
      "n_p_ecg_p_04 is unbalanced. 1695 5\n",
      "n_p_ecg_p_05 is unbalanced. 1698 2\n",
      "n_p_ecg_p_06 is balanced. 1673 27\n",
      "n_p_ecg_p_07 is balanced. 1598 102\n",
      "n_p_ecg_p_08 is unbalanced. 1693 7\n",
      "n_p_ecg_p_09 is unbalanced. 1690 10\n",
      "n_p_ecg_p_10 is balanced. 1666 34\n",
      "n_p_ecg_p_11 is balanced. 1672 28\n",
      "n_p_ecg_p_12 is balanced. 1622 78\n",
      "fibr_ter_01 is unbalanced. 1687 13\n",
      "fibr_ter_02 is unbalanced. 1684 16\n",
      "fibr_ter_03 is balanced. 1632 68\n",
      "fibr_ter_05 is unbalanced. 1696 4\n",
      "fibr_ter_06 is unbalanced. 1691 9\n",
      "fibr_ter_07 is unbalanced. 1694 6\n",
      "fibr_ter_08 is unbalanced. 1698 2\n",
      "GIPER_NA is balanced. 1670 30\n",
      "NITR_S is balanced. 1505 195\n",
      "LID_S_n is balanced. 1219 481\n",
      "B_BLOK_S_n is balanced. 1485 215\n",
      "ANT_CA_S_n is balanced. 565 1135\n",
      "GEPAR_S_n is balanced. 484 1216\n",
      "ASP_S_n is balanced. 434 1266\n",
      "TIKL_S_n is balanced. 1670 30\n",
      "TRENT_S_n is balanced. 1359 341\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "binary_column_info = {}\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "for column in x.columns:\n",
    "    # 유일한 값들 확인\n",
    "    unique_values = x[column].dropna().unique()\n",
    "    \n",
    "    # 0과 1만 있는지 확인\n",
    "    if set(unique_values) <= {0, 1}:\n",
    "        count_0 = (x[column] == 0).sum()\n",
    "        count_1 = (x[column] == 1).sum()\n",
    "        total_count = count_0 + count_1\n",
    "        percent_0 = (count_0 / total_count) * 100\n",
    "        percent_1 = (count_1 / total_count) * 100\n",
    "        binary_column_info[column] = {\n",
    "            '0 count': count_0, \n",
    "            '1 count': count_1,\n",
    "            '0 percent': percent_0,\n",
    "            '1 percent': percent_1\n",
    "        }\n",
    "\n",
    "li = []\n",
    "for column, counts in binary_column_info.items():\n",
    "\n",
    "    if(counts['1 percent'] <= 1):\n",
    "        print(column, \"is unbalanced.\", counts['0 count'], counts['1 count'])\n",
    "        cnt +=1\n",
    "        li.append(column)\n",
    "    else:\n",
    "        print(column, \"is balanced.\", counts['0 count'], counts['1 count'])\n",
    "\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1700, 61)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.drop(columns=li)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.to_csv('./1_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. VAE Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = pd.read_csv('./1_1.csv')\n",
    "data = data.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(data)\n",
    "\n",
    "# Convert normalized data back to a DataFrame\n",
    "normalized_df = pd.DataFrame(normalized_data, columns=data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0   AGE  SEX  INF_ANAM  IBS_POST  SIM_GIPERT  ZSN_A  nr_11  nr_02  \\\n",
      "0           0  77.0    1         2         2           0      0      0      0   \n",
      "1           1  55.0    1         1         0           0      0      0      0   \n",
      "2           2  52.0    1         0         2           0      0      0      0   \n",
      "3           3  68.0    0         0         2           0      1      0      0   \n",
      "4           4  60.0    1         0         2           0      0      0      0   \n",
      "\n",
      "   nr_03  ...  NOT_NA_1_n  NOT_NA_2_n  NOT_NA_3_n  LID_S_n  B_BLOK_S_n  \\\n",
      "0      0  ...           0           0           0        1           0   \n",
      "1      0  ...           1           0           0        1           0   \n",
      "2      0  ...           3           2           2        1           1   \n",
      "3      0  ...           0           0           0        0           0   \n",
      "4      0  ...           0           0           0        0           0   \n",
      "\n",
      "   ANT_CA_S_n  GEPAR_S_n  ASP_S_n  TIKL_S_n  TRENT_S_n  \n",
      "0           0          1        1         0          0  \n",
      "1           1          1        1         0          1  \n",
      "2           0          1        1         0          0  \n",
      "3           1          1        1         0          0  \n",
      "4           1          0        1         0          1  \n",
      "\n",
      "[5 rows x 62 columns]\n",
      "WARNING:tensorflow:Output dense_147 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_147.\n",
      "Train on 1360 samples, validate on 340 samples\n",
      "Epoch 1/50\n",
      "1360/1360 [==============================] - 2s 1ms/sample - loss: 74.2598 - val_loss: 69.3344\n",
      "Epoch 2/50\n",
      "1360/1360 [==============================] - 0s 59us/sample - loss: 65.5990 - val_loss: 62.4748\n",
      "Epoch 3/50\n",
      "1360/1360 [==============================] - 0s 53us/sample - loss: 61.9876 - val_loss: 61.3767\n",
      "Epoch 4/50\n",
      "1360/1360 [==============================] - 0s 55us/sample - loss: 61.4030 - val_loss: 61.0214\n",
      "Epoch 5/50\n",
      "1360/1360 [==============================] - 0s 56us/sample - loss: 61.2135 - val_loss: 60.9127\n",
      "Epoch 6/50\n",
      "1360/1360 [==============================] - 0s 62us/sample - loss: 61.1855 - val_loss: 60.8542\n",
      "Epoch 7/50\n",
      "1360/1360 [==============================] - 0s 66us/sample - loss: 61.1562 - val_loss: 60.8581\n",
      "Epoch 8/50\n",
      "1360/1360 [==============================] - 0s 51us/sample - loss: 61.1343 - val_loss: 60.7757\n",
      "Epoch 9/50\n",
      "1360/1360 [==============================] - 0s 52us/sample - loss: 61.1311 - val_loss: 60.8077\n",
      "Epoch 10/50\n",
      "1360/1360 [==============================] - 0s 50us/sample - loss: 61.1310 - val_loss: 60.7822\n",
      "Epoch 11/50\n",
      "1360/1360 [==============================] - 0s 66us/sample - loss: 61.1043 - val_loss: 60.8052\n",
      "Epoch 12/50\n",
      "1360/1360 [==============================] - 0s 67us/sample - loss: 61.0933 - val_loss: 60.7452\n",
      "Epoch 13/50\n",
      "1360/1360 [==============================] - 0s 121us/sample - loss: 61.1024 - val_loss: 60.7535\n",
      "Epoch 14/50\n",
      "1360/1360 [==============================] - 0s 74us/sample - loss: 61.0943 - val_loss: 60.7678\n",
      "Epoch 15/50\n",
      "1360/1360 [==============================] - 0s 56us/sample - loss: 61.0907 - val_loss: 60.7544\n",
      "Epoch 16/50\n",
      "1360/1360 [==============================] - 0s 60us/sample - loss: 61.0841 - val_loss: 60.7540\n",
      "Epoch 17/50\n",
      "1360/1360 [==============================] - 0s 53us/sample - loss: 61.0946 - val_loss: 60.7672\n",
      "Epoch 18/50\n",
      "1360/1360 [==============================] - 0s 60us/sample - loss: 61.0790 - val_loss: 60.7820\n",
      "Epoch 19/50\n",
      "1360/1360 [==============================] - 0s 54us/sample - loss: 61.0823 - val_loss: 60.7585\n",
      "Epoch 20/50\n",
      "1360/1360 [==============================] - 0s 46us/sample - loss: 61.0862 - val_loss: 60.7442\n",
      "Epoch 21/50\n",
      "1360/1360 [==============================] - 0s 51us/sample - loss: 61.0812 - val_loss: 60.7361\n",
      "Epoch 22/50\n",
      "1360/1360 [==============================] - 0s 52us/sample - loss: 61.0814 - val_loss: 60.7549\n",
      "Epoch 23/50\n",
      "1360/1360 [==============================] - 0s 49us/sample - loss: 61.0869 - val_loss: 60.7706\n",
      "Epoch 24/50\n",
      "1360/1360 [==============================] - 0s 49us/sample - loss: 61.0647 - val_loss: 60.7444\n",
      "Epoch 25/50\n",
      "1360/1360 [==============================] - 0s 49us/sample - loss: 61.0922 - val_loss: 60.7688\n",
      "Epoch 26/50\n",
      "1360/1360 [==============================] - 0s 46us/sample - loss: 61.0785 - val_loss: 60.7250\n",
      "Epoch 27/50\n",
      "1360/1360 [==============================] - 0s 47us/sample - loss: 61.0745 - val_loss: 60.7577\n",
      "Epoch 28/50\n",
      "1360/1360 [==============================] - 0s 48us/sample - loss: 61.0811 - val_loss: 60.7424\n",
      "Epoch 29/50\n",
      "1360/1360 [==============================] - 0s 46us/sample - loss: 61.0699 - val_loss: 60.7686\n",
      "Epoch 30/50\n",
      "1360/1360 [==============================] - 0s 46us/sample - loss: 61.0751 - val_loss: 60.7599\n",
      "Epoch 31/50\n",
      "1360/1360 [==============================] - 0s 43us/sample - loss: 61.0705 - val_loss: 60.7191\n",
      "Epoch 32/50\n",
      "1360/1360 [==============================] - 0s 46us/sample - loss: 61.0812 - val_loss: 60.7481\n",
      "Epoch 33/50\n",
      "1360/1360 [==============================] - 0s 65us/sample - loss: 61.0662 - val_loss: 60.7389\n",
      "Epoch 34/50\n",
      "1360/1360 [==============================] - 0s 55us/sample - loss: 61.0655 - val_loss: 60.7304\n",
      "Epoch 35/50\n",
      "1360/1360 [==============================] - 0s 49us/sample - loss: 61.0670 - val_loss: 60.7404\n",
      "Epoch 36/50\n",
      "1360/1360 [==============================] - 0s 46us/sample - loss: 61.0711 - val_loss: 60.7684\n",
      "Epoch 37/50\n",
      "1360/1360 [==============================] - 0s 45us/sample - loss: 61.0736 - val_loss: 60.7489\n",
      "Epoch 38/50\n",
      "1360/1360 [==============================] - 0s 47us/sample - loss: 61.0654 - val_loss: 60.7521\n",
      "Epoch 39/50\n",
      "1360/1360 [==============================] - 0s 49us/sample - loss: 61.0600 - val_loss: 60.7608\n",
      "Epoch 40/50\n",
      "1360/1360 [==============================] - 0s 44us/sample - loss: 61.0501 - val_loss: 60.7299\n",
      "Epoch 41/50\n",
      "1360/1360 [==============================] - 0s 71us/sample - loss: 61.0543 - val_loss: 60.7380\n",
      "Epoch 42/50\n",
      "1360/1360 [==============================] - 0s 73us/sample - loss: 61.0631 - val_loss: 60.7453\n",
      "Epoch 43/50\n",
      "1360/1360 [==============================] - 0s 55us/sample - loss: 61.0171 - val_loss: 60.7821\n",
      "Epoch 44/50\n",
      "1360/1360 [==============================] - 0s 52us/sample - loss: 61.0353 - val_loss: 60.7188\n",
      "Epoch 45/50\n",
      "1360/1360 [==============================] - 0s 51us/sample - loss: 61.0631 - val_loss: 60.7613\n",
      "Epoch 46/50\n",
      "1360/1360 [==============================] - 0s 54us/sample - loss: 61.0156 - val_loss: 60.7474\n",
      "Epoch 47/50\n",
      "1360/1360 [==============================] - 0s 51us/sample - loss: 61.0402 - val_loss: 60.7047\n",
      "Epoch 48/50\n",
      "1360/1360 [==============================] - 0s 51us/sample - loss: 61.0031 - val_loss: 60.6667\n",
      "Epoch 49/50\n",
      "1360/1360 [==============================] - 0s 49us/sample - loss: 60.9412 - val_loss: 60.6688\n",
      "Epoch 50/50\n",
      "1360/1360 [==============================] - 0s 56us/sample - loss: 60.9465 - val_loss: 60.6047\n",
      "         AGE       SEX  INF_ANAM  IBS_POST  SIM_GIPERT     ZSN_A     nr_11  \\\n",
      "0  61.862152  0.626471  0.555438  1.167690    0.033530  0.191783  0.024706   \n",
      "1  61.904064  0.626480  0.560357  1.177472    0.033595  0.194831  0.024737   \n",
      "2  61.873535  0.626479  0.562378  1.171458    0.033598  0.194011  0.024729   \n",
      "3  61.862202  0.626471  0.555547  1.167729    0.033530  0.191801  0.024706   \n",
      "4  61.887348  0.626481  0.562495  1.174050    0.033603  0.195387  0.024732   \n",
      "\n",
      "      nr_02     nr_03     nr_04  ...  NOT_NA_1_n  NOT_NA_2_n  NOT_NA_3_n  \\\n",
      "0  0.011176  0.020589  0.017077  ...    0.331804    0.107077    0.078243   \n",
      "1  0.011191  0.020812  0.018894  ...    0.338323    0.112066    0.080915   \n",
      "2  0.011188  0.020753  0.018038  ...    0.335211    0.109224    0.079351   \n",
      "3  0.011176  0.020590  0.017092  ...    0.331840    0.107095    0.078251   \n",
      "4  0.011200  0.020795  0.018277  ...    0.340012    0.114703    0.081042   \n",
      "\n",
      "    LID_S_n  B_BLOK_S_n  ANT_CA_S_n  GEPAR_S_n   ASP_S_n  TIKL_S_n  TRENT_S_n  \n",
      "0  0.282947    0.126471    0.667647   0.715295  0.744706  0.017647   0.200590  \n",
      "1  0.284451    0.126698    0.668007   0.715472  0.744713  0.017666   0.200874  \n",
      "2  0.283917    0.126532    0.667853   0.715510  0.744718  0.017673   0.200948  \n",
      "3  0.282953    0.126471    0.667648   0.715295  0.744706  0.017647   0.200591  \n",
      "4  0.284557    0.126687    0.667890   0.715533  0.744718  0.017668   0.200960  \n",
      "\n",
      "[5 rows x 61 columns]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, backend as K\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data\n",
    "file_path = './1_1.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the data\n",
    "print(data.head())\n",
    "\n",
    "# Preprocess the data\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data.drop(columns=['Unnamed: 0']))\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "x_train, x_val = train_test_split(data_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the encoder\n",
    "input_dim = x_train.shape[1]\n",
    "latent_dim = 2  # You can adjust the dimensionality of the latent space\n",
    "\n",
    "inputs = layers.Input(shape=(input_dim,))\n",
    "h = layers.Dense(64, activation='relu')(inputs)\n",
    "h = layers.Dense(32, activation='relu')(h)\n",
    "z_mean = layers.Dense(latent_dim)(h)\n",
    "z_log_var = layers.Dense(latent_dim)(h)\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = tf.shape(z_mean)[0]\n",
    "    dim = tf.shape(z_mean)[1]\n",
    "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "    return z_mean + tf.keras.backend.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = layers.Lambda(sampling)([z_mean, z_log_var])\n",
    "\n",
    "# Define the decoder\n",
    "decoder_input = layers.Input(shape=(latent_dim,))\n",
    "decoder_h = layers.Dense(32, activation='relu')\n",
    "decoder_h2 = layers.Dense(64, activation='relu')\n",
    "decoder_mean = layers.Dense(input_dim, activation='sigmoid')\n",
    "\n",
    "_h_decoded = decoder_h(decoder_input)\n",
    "_h_decoded = decoder_h2(_h_decoded)\n",
    "_outputs = decoder_mean(_h_decoded)\n",
    "\n",
    "decoder = models.Model(decoder_input, _outputs)\n",
    "\n",
    "# Define the VAE model\n",
    "h_decoded = decoder_h(z)\n",
    "h_decoded = decoder_h2(h_decoded)\n",
    "outputs = decoder_mean(h_decoded)\n",
    "vae = models.Model(inputs, outputs)\n",
    "\n",
    "# Define the loss\n",
    "reconstruction_loss = tf.keras.losses.mse(inputs, outputs)\n",
    "reconstruction_loss *= input_dim\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "\n",
    "# Train the VAE model\n",
    "vae.fit(x_train, epochs=50, batch_size=32, validation_data=(x_val, None))\n",
    "\n",
    "# Generate new samples using the trained VAE\n",
    "def generate_samples(decoder, num_samples, latent_dim):\n",
    "    z_sample = np.random.normal(size=(num_samples, latent_dim))\n",
    "    generated_data = decoder.predict(z_sample)\n",
    "    return scaler.inverse_transform(generated_data)\n",
    "\n",
    "# Generate 100 new samples\n",
    "new_samples = generate_samples(decoder, 10, latent_dim)\n",
    "\n",
    "# Convert generated samples to DataFrame and round all values to integers\n",
    "new_samples_df = pd.DataFrame(new_samples, columns=data.columns[1:])\n",
    "#new_samples_df = new_samples_df.round(1)\n",
    "\n",
    "# Display the generated samples\n",
    "print(new_samples_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGE [61.862152 61.904064 61.873535 61.8622   61.88735  61.86219  62.29251\n",
      " 61.872353 61.862274 61.862232]\n",
      "SEX [0.62647057 0.6264798  0.6264786  0.6264807  0.6273904  0.6264753 ]\n",
      "INF_ANAM [0.5554382  0.56035745 0.56237835 0.5555472  0.5624952  0.555519\n",
      " 0.60098267 0.5607117  0.5557131  0.5556157 ]\n",
      "IBS_POST [1.1676898 1.177472  1.1714579 1.1677295 1.1740502 1.1677223 1.2279601\n",
      " 1.1714482 1.1677824 1.1677508]\n",
      "SIM_GIPERT [0.03352957 0.0335954  0.03359849 0.0335298  0.0336033  0.03352977\n",
      " 0.03554852 0.03359427 0.03353011 0.03352992]\n",
      "ZSN_A [0.19178294 0.19483143 0.19401135 0.19180085 0.19538674 0.19179365\n",
      " 0.21972127 0.19313085 0.19183792 0.19181687]\n",
      "nr_11 [0.02470591 0.02473683 0.02472922 0.02470594 0.0247322  0.02470592\n",
      " 0.02584688 0.02472484 0.02470601 0.02470597]\n",
      "nr_02 [0.01117648 0.01119135 0.01118796 0.01117649 0.01119956 0.01117649\n",
      " 0.01187023 0.01118534 0.01117652 0.0111765 ]\n",
      "nr_03 [0.02058899 0.02081238 0.02075322 0.02058991 0.02079522 0.02058982\n",
      " 0.0244072  0.02075441 0.02059102 0.02059035]\n",
      "nr_04 [0.01707719 0.01889387 0.0180377  0.01709191 0.01827713 0.01709061\n",
      " 0.02829297 0.01815072 0.01710585 0.01709781]\n",
      "endocr_01 [0.13416615 0.13667941 0.1366619  0.13420418 0.13681042 0.13419554\n",
      " 0.15610197 0.13619864 0.1342548  0.13422632]\n",
      "endocr_02 [0.02470589 0.02472172 0.02471464 0.0247059  0.02471772 0.02541381\n",
      " 0.02471217 0.02470591 0.0247059 ]\n",
      "zab_leg_01 [0.08000004 0.08014601 0.08005009 0.08000011 0.08007447 0.08000009\n",
      " 0.08354428 0.08004207 0.08000027 0.08000015]\n",
      "zab_leg_02 [0.07118257 0.07181512 0.0718658  0.0711887  0.07189841 0.07118772\n",
      " 0.0802618  0.07169726 0.07119616 0.07119195]\n",
      "zab_leg_03 [0.02176474 0.02180433 0.02178091 0.02176474 0.02178677 0.02176473\n",
      " 0.02304543 0.02178063 0.02176477 0.02176475]\n",
      "zab_leg_06 [0.01294118 0.01294606 0.01294359 0.01294426 0.0133153  0.01294352\n",
      " 0.01294119 0.01294118]\n",
      "O_L_POST [0.06474879 0.06703202 0.06694829 0.06478121 0.06844371 0.06476694\n",
      " 0.08102103 0.06605907 0.06484473 0.06481044]\n",
      "K_SH_POST [0.02705886 0.02710824 0.02709256 0.02705892 0.02710214 0.02882509\n",
      " 0.02708673 0.02705902 0.02705896]\n",
      "ant_im [1.5576988 1.5651863 1.5632585 1.5577499 1.5641593 1.5577384 1.6381831\n",
      " 1.56226   1.5578233 1.5577807]\n",
      "lat_im [0.8529423  0.85469455 0.8534702  0.852944   0.8539051  0.8529435\n",
      " 0.8747314  0.8533386  0.85294706 0.85294527]\n",
      "inf_im [1.0117804 1.0132588 1.0139536 1.0117972 1.0130275 1.0117947 1.0391722\n",
      " 1.0138856 1.011819  1.011806 ]\n",
      "post_im [0.2541177  0.25430527 0.25420427 0.25411776 0.25425553 0.26033825\n",
      " 0.2541813  0.25411803 0.2541179 ]\n",
      "IM_PG_P [0.02941343 0.02972433 0.02969345 0.02941529 0.02971346 0.0294148\n",
      " 0.03389983 0.02963049 0.02941842 0.02941656]\n",
      "ritm_ecg_p_04 [0.01353086 0.0138203  0.01377406 0.01353243 0.01386746 0.01353185\n",
      " 0.01701835 0.01368634 0.01353573 0.01353378]\n",
      "ritm_ecg_p_08 [0.02705883 0.02708621 0.02707102 0.02705884 0.02707676 0.02705885\n",
      " 0.02811645 0.02706604 0.02705886 0.02705885]\n",
      "n_r_ecg_p_01 [0.03411766 0.03413134 0.03413098 0.03411767 0.03413999 0.03411767\n",
      " 0.0349233  0.03412516 0.03411768]\n",
      "n_r_ecg_p_03 [0.12176739 0.12282499 0.12228005 0.12177053 0.12247363 0.12176988\n",
      " 0.13334772 0.12225494 0.12177495 0.12177243]\n",
      "n_r_ecg_p_04 [0.04058824 0.04063711 0.04061018 0.04058827 0.04063261 0.04058826\n",
      " 0.04223591 0.04060145 0.04058832 0.04058828]\n",
      "n_r_ecg_p_05 [0.04119041 0.04231163 0.04218166 0.04120248 0.04218311 0.04119933\n",
      " 0.05123722 0.04198308 0.04122162 0.04121027]\n",
      "n_r_ecg_p_06 [0.01884081 0.01956951 0.01966431 0.01885427 0.01945597 0.01885288\n",
      " 0.02577492 0.01969538 0.0188688  0.01885999]\n",
      "n_p_ecg_p_03 [0.01882361 0.01885075 0.01887152 0.01882374 0.01887813 0.01882371\n",
      " 0.01999116 0.01885411 0.01882397 0.01882383]\n",
      "n_p_ecg_p_06 [0.01588396 0.01631309 0.01613959 0.01588572 0.01623551 0.01588536\n",
      " 0.02063907 0.01611349 0.01588841 0.0158868 ]\n",
      "n_p_ecg_p_07 [0.06000044 0.06024368 0.06016548 0.06000104 0.06021378 0.06000086\n",
      " 0.06430876 0.06011593 0.06000219 0.0600015 ]\n",
      "n_p_ecg_p_10 [0.02000008 0.02009639 0.02006136 0.02000021 0.02017619 0.02000017\n",
      " 0.02217175 0.02003283 0.02000052 0.02000034]\n",
      "n_p_ecg_p_11 [0.01647736 0.01695366 0.0169835  0.01648335 0.01700849 0.01648199\n",
      " 0.02147382 0.01687408 0.01649132 0.01648689]\n",
      "n_p_ecg_p_12 [0.04588246 0.04598846 0.04596328 0.04588262 0.04603918 0.04588255\n",
      " 0.04836836 0.04592184 0.0458831  0.04588281]\n",
      "fibr_ter_03 [0.04       0.04000649 0.04000594 0.04000001 0.04000532 0.04000001\n",
      " 0.04052386 0.0400039  0.04000002]\n",
      "GIPER_NA [0.01764706 0.01765853 0.01765598 0.01764707 0.01767204 0.0183573\n",
      " 0.01765162 0.01764709 0.01764708]\n",
      "NA_BLOOD [136.60365 136.6038  136.60373 136.60376 136.61949]\n",
      "ALT_BLOOD [0.47570124 0.47587737 0.47575903 0.47570127 0.47583738 0.48039863\n",
      " 0.4757424  0.47570145 0.47570133]\n",
      "AST_BLOOD [0.25876117 0.2588027  0.2587824  0.2587612  0.25879523 0.26037076\n",
      " 0.2587779  0.25876126 0.25876123]\n",
      "L_BLOOD [8.737515 8.739736 8.739271 8.737522 8.740168 8.73752  8.787351 8.738888\n",
      " 8.737532 8.737526]\n",
      "ROE [13.525304 13.531178 13.530332 13.525319 13.53033  13.525313 13.658932\n",
      " 13.528406 13.525348 13.525331]\n",
      "TIME_B_S [4.7382355 4.7382894 4.738268  4.7382865 4.7433896 4.7382526]\n",
      "R_AB_1_n [0.3147642  0.33129433 0.319172   0.3148162  0.3257705  0.31481254\n",
      " 0.40146783 0.31964603 0.31486672 0.3148365 ]\n",
      "R_AB_2_n [0.13652629 0.14676358 0.14064096 0.1365735  0.15034884 0.13655686\n",
      " 0.19025895 0.1390205  0.13665354 0.13661015]\n",
      "R_AB_3_n [0.07178321 0.07451387 0.07355691 0.07180008 0.07667983 0.07179289\n",
      " 0.09347042 0.07274412 0.07183341 0.0718155 ]\n",
      "NITR_S [0.11477749 0.11872265 0.1180552  0.11483168 0.1215165  0.11482801\n",
      " 0.14373685 0.11778483 0.11488318 0.114852  ]\n",
      "NA_R_1_n [0.48365945 0.50876373 0.4910978  0.48376456 0.49991634 0.48374522\n",
      " 0.59300643 0.49045178 0.48388535 0.4838193 ]\n",
      "NA_R_2_n [0.08883181 0.09391403 0.09012067 0.0888407  0.09362659 0.08883887\n",
      " 0.12280085 0.08988778 0.08885337 0.08884598]\n",
      "NA_R_3_n [0.05412362 0.05709951 0.05505872 0.05412992 0.05773159 0.05412791\n",
      " 0.07653768 0.05475583 0.05414122 0.05413475]\n",
      "NOT_NA_1_n [0.33180416 0.33832267 0.33521116 0.3318399  0.34001243 0.33183053\n",
      " 0.3819154  0.33424717 0.33189318 0.3318631 ]\n",
      "NOT_NA_2_n [0.10707735 0.11206584 0.10922446 0.10709519 0.11470255 0.10708861\n",
      " 0.1418453  0.10833428 0.10713018 0.10711004]\n",
      "NOT_NA_3_n [0.07824305 0.08091526 0.07935113 0.07825124 0.08104165 0.07824978\n",
      " 0.10313424 0.07916167 0.0782614  0.0782557 ]\n",
      "LID_S_n [0.28294683 0.2844511  0.2839169  0.28295296 0.28455687 0.28295088\n",
      " 0.3002156  0.28361216 0.28296432 0.28295794]\n",
      "B_BLOK_S_n [0.12647063 0.12669846 0.12653199 0.1264707  0.1266867  0.12647067\n",
      " 0.13184057 0.12651831 0.12647085 0.12647077]\n",
      "ANT_CA_S_n [0.6676474  0.6680073  0.6678531  0.6676481  0.6678905  0.6676479\n",
      " 0.67461705 0.6677789  0.66764927 0.66764855]\n",
      "GEPAR_S_n [0.7152946  0.7154717  0.71550965 0.71529526 0.71553254 0.7152951\n",
      " 0.7205643  0.7154516  0.71529645 0.71529573]\n",
      "ASP_S_n [0.74470586 0.74471295 0.7447185  0.74471796 0.7447059  0.7455358\n",
      " 0.7447125 ]\n",
      "TIKL_S_n [0.01764708 0.01766647 0.01767261 0.01764713 0.01766755 0.01764712\n",
      " 0.01848576 0.01766602 0.01764722 0.01764718]\n",
      "TRENT_S_n [0.20058964 0.20087409 0.20094822 0.20059143 0.20095995 0.20059106\n",
      " 0.20700115 0.20085245 0.20059401 0.20059252]\n"
     ]
    }
   ],
   "source": [
    "#값 분포 확인\n",
    "for column in new_samples_df.columns:\n",
    "    unique_values = new_samples_df[column].dropna().unique()\n",
    "    print(column, unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([data, new_samples_df], ignore_index=True)\n",
    "combined_df = combined_df.drop(columns=['Unnamed: 0'])\n",
    "combined_df.to_csv('1_2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
